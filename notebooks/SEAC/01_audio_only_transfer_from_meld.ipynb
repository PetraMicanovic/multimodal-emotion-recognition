{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOuO3rhuUrJD"
      },
      "source": [
        "# Transfer learning for Speech Emotion Recognition on Serbian acted speech\n",
        "\n",
        "In this notebook, we evaluate a pretrained audio emotion recognition model on a newly collected\n",
        "Serbian acted speech dataset. The experiments are conducted in three stages:\n",
        "\n",
        "1. Baseline evaluation of the pretrained model on two unseen Serbian speakers.\n",
        "2. Fine-tuning the model using the remaining Serbian speakers.\n",
        "3. Re-evaluation on both Serbian and original English test sets.\n",
        "\n",
        "Only the audio modality is considered, as the textual content is not correlated with emotion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paVSsEjfN502"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R33zhUU7lBY",
        "outputId": "12644312-6f90-463f-a385-8ac3507d7ba5"
      },
      "outputs": [],
      "source": [
        "# System setup and dependencies\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install ffmpeg\n",
        "!pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 --index-url https://download.pytorch.org/whl/cu126"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AjCX2je7lBZ",
        "outputId": "7ce85abd-9cfc-4861-bc75-05fc0fc868b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchcodec==0.8.0\n",
            "  Downloading torchcodec-0.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Downloading torchcodec-0.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.5/1.9 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.8.0\n"
          ]
        }
      ],
      "source": [
        "# Additional Python packages for AI/Audio/NLP\n",
        "\n",
        "!pip -q install transformers datasets accelerate sentencepiece\n",
        "\n",
        "!pip install -q librosa soundfile\n",
        "\n",
        "!pip install torchcodec==0.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jkwuc1l7UfwQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De7y_b71R3NF",
        "outputId": "a4e2ce7d-2767-415b-f547-63112272bf3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google Drive is used to store the MELD dataset and to save the processed audio embeddings.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yj_5mMR1RLZ"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p2sGkp1AKXrK"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # ========= DEVICE =========\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "\n",
        "    # ========= PATHS =========\n",
        "    \"seac_root\": Path(\"/content/drive/MyDrive/datasets/data_mono\"),\n",
        "    \"meld_checkpoint\": Path(\"/content/drive/MyDrive/models/checkpoints/audio_best.pt\"),\n",
        "\n",
        "    # ========= DATA =========\n",
        "    \"sample_rate\": 16000,\n",
        "    \"max_audio_length\": 4.0,\n",
        "\n",
        "    # ===== EMOTIONS (APH–SEAC) =====\n",
        "    \"seac_emotion_map\": {\n",
        "        \"0\": \"anger\",\n",
        "        \"1\": \"neutral\",\n",
        "        \"2\": \"happiness\",\n",
        "        \"3\": \"fear\",\n",
        "        \"4\": \"sadness\",\n",
        "    },\n",
        "\n",
        "   # Normalize SEAC → canonical names (MELD-style)\n",
        "    \"emotion_normalization\": {\n",
        "        \"happiness\": \"joy\",\n",
        "        \"neutral\": \"neutral\",\n",
        "        \"anger\": \"anger\",\n",
        "        \"sadness\": \"sadness\",\n",
        "        \"fear\": \"fear\",\n",
        "    },\n",
        "\n",
        "    # MELD label mapping\n",
        "    \"meld_label_map\": {\n",
        "        \"neutral\": 0,\n",
        "        \"joy\": 1,\n",
        "        \"anger\": 3,\n",
        "        \"sadness\": 4,\n",
        "        \"fear\": 5,\n",
        "    },\n",
        "\n",
        "    # ========= Speakers =========\n",
        "    \"selected_speakers\": {\n",
        "        \"0005\":\"female\",\n",
        "        \"1005\":\"male\",\n",
        "    },\n",
        "\n",
        "    # Model\n",
        "    \"audio_embedding_dim\": 768,\n",
        "    \"num_classes\": 7,\n",
        "    \"dropout\": 0.3,\n",
        "\n",
        "\n",
        "    # ========= TRAINING =========\n",
        "    \"batch_size\": 16,\n",
        "    \"lr_finetune\": 1e-4,\n",
        "    \"num_epochs\": 30,\n",
        "    \"early_stopping_patience\": 5,\n",
        "\n",
        "    # ========= REPRODUCIBILITY =========\n",
        "    \"seed\": 42,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nED-MCnBVEpP"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(CONFIG[\"seed\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VjYoYHltaFSE"
      },
      "outputs": [],
      "source": [
        "label_id_to_name = {\n",
        "    0: \"neutral\",\n",
        "    1: \"joy\",\n",
        "    2: \"surprise\",\n",
        "    3: \"anger\",\n",
        "    4: \"sadness\",\n",
        "    5: \"fear\",\n",
        "    6: \"disgust\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_h6FGVL1V6P"
      },
      "source": [
        "## SEAC  Dataset\n",
        "\n",
        "The SEAC dataset consists of emotionally expressive speech recordings in Serbian, organized hierarchically by speakers and emotion categories. Each speaker is represented by a separate directory containing multiple subfolders, where each subfolder corresponds to a specific emotion and includes several .wav audio files.\n",
        "\n",
        "For the experiments in this work, a subset of the SEAC dataset is used, defined by a selected set of speakers.\n",
        "\n",
        "Audio samples are loaded by iterating over the selected speakers and their emotion-specific subfolders. Emotion labels are first normalized and then mapped to the MELD emotion label space in order to ensure label consistency across datasets. Each sample is stored together with its audio path, normalized emotion, numerical class label, and speaker identifier.\n",
        "\n",
        "To enable model evaluation, a custom PyTorch dataset is defined for audio processing. All audio signals are converted to mono, resampled to 16 kHz, and truncated to a fixed maximum duration. The test set consists of all loaded SEAC samples and is evaluated using a DataLoader with a batch size of one, allowing sample-level analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0zx_o0b0ege"
      },
      "outputs": [],
      "source": [
        "def load_seac_samples(config, speakers):\n",
        "    samples = []\n",
        "\n",
        "    for speaker_id in speakers:\n",
        "        speaker_path = config[\"seac_root\"] / speaker_id\n",
        "\n",
        "        for folder_id, emotion in config[\"seac_emotion_map\"].items():\n",
        "            emotion_path = speaker_path / folder_id\n",
        "            if not emotion_path.exists():\n",
        "                continue\n",
        "\n",
        "            for wav in emotion_path.glob(\"*.wav\"):\n",
        "\n",
        "                # 1. normalize emotion name\n",
        "                norm_emotion = config[\"emotion_normalization\"][emotion]\n",
        "\n",
        "                # 2. map to MELD label\n",
        "                meld_label = config[\"meld_label_map\"][norm_emotion]\n",
        "\n",
        "                samples.append({\n",
        "                    \"audio_path\": wav,\n",
        "                    \"emotion\": norm_emotion,\n",
        "                    \"label\": meld_label,\n",
        "                    \"speaker\": speaker_id\n",
        "                })\n",
        "\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o5LnApPl0kti"
      },
      "outputs": [],
      "source": [
        "class SEACAudioDataset(Dataset):\n",
        "    def __init__(self, samples, sample_rate=16000, max_len=4.0):\n",
        "        self.samples = samples\n",
        "        self.sample_rate = sample_rate\n",
        "        self.max_len = int(sample_rate * max_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        path = sample[\"audio_path\"]\n",
        "        label = sample[\"label\"]\n",
        "\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(path)\n",
        "\n",
        "            if sr != self.sample_rate:\n",
        "                waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
        "\n",
        "            waveform = waveform.squeeze(0)\n",
        "\n",
        "            if waveform.shape[0] > self.max_len:\n",
        "                waveform = waveform[:self.max_len]\n",
        "            else:\n",
        "                pad = self.max_len - waveform.shape[0]\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, pad))\n",
        "\n",
        "        except Exception:\n",
        "            waveform = torch.zeros(self.max_len, dtype=torch.float32)\n",
        "\n",
        "        return waveform, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgvaYC6hsRsi"
      },
      "outputs": [],
      "source": [
        "def build_audio_loader(\n",
        "    samples,\n",
        "    config,\n",
        "    batch_size,\n",
        "    shuffle,\n",
        "    collate_fn=None\n",
        "):\n",
        "    dataset = SEACAudioDataset(\n",
        "        samples,\n",
        "        sample_rate=config[\"sample_rate\"],\n",
        "        max_len=config[\"max_audio_length\"]\n",
        "    )\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RfkAIhTs0ruD"
      },
      "outputs": [],
      "source": [
        "def audio_collate_fn(batch):\n",
        "    waveforms, labels = zip(*batch)\n",
        "\n",
        "    waveforms = [\n",
        "        torch.tensor(w, dtype=torch.float32)\n",
        "        if not torch.is_tensor(w) else w\n",
        "        for w in waveforms\n",
        "    ]\n",
        "\n",
        "    waveforms = torch.nn.utils.rnn.pad_sequence(\n",
        "        waveforms,\n",
        "        batch_first=True\n",
        "    )\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return waveforms, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tundTb4pdGt",
        "outputId": "5583e46b-6057-4944-ed38-1b73eb122566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 598 SEAC test samples\n",
            "Counter({0: 122, 5: 122, 4: 122, 3: 116, 1: 116})\n",
            "Counter({'neutral': 122, 'fear': 122, 'sadness': 122, 'anger': 116, 'joy': 116})\n"
          ]
        }
      ],
      "source": [
        "samples_step1 = load_seac_samples(CONFIG,CONFIG[\"selected_speakers\"])\n",
        "print(f\"Loaded {len(samples_step1)} SEAC test samples\")\n",
        "\n",
        "from collections import Counter\n",
        "print(Counter([s[\"label\"] for s in samples_step1]))\n",
        "print(Counter([s[\"emotion\"] for s in samples_step1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "y4YFSgy-VHLX"
      },
      "outputs": [],
      "source": [
        "test_loader = build_audio_loader(\n",
        "    samples=samples_step1,\n",
        "    config=CONFIG,\n",
        "    batch_size=1,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "941kUa1SEbNP"
      },
      "source": [
        "## Audio Feature Extraction\n",
        "\n",
        "A pre-trained Wav2Vec 2.0 base model is used to extract audio embeddings from raw waveforms.\n",
        "The model is initialized with pre-trained weights and used in evaluation mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAtG7-TPbxx8",
        "outputId": "470ecf70-dd02-42f7-bcbb-64748a973774"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Wav2Vec2Model(\n",
              "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
              "    (conv_layers): ModuleList(\n",
              "      (0): Wav2Vec2GroupNormConvLayer(\n",
              "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
              "        (activation): GELUActivation()\n",
              "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
              "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
              "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (feature_projection): Wav2Vec2FeatureProjection(\n",
              "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): Wav2Vec2Encoder(\n",
              "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
              "      (conv): ParametrizedConv1d(\n",
              "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
              "        (parametrizations): ModuleDict(\n",
              "          (weight): ParametrizationList(\n",
              "            (0): _WeightNorm()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (padding): Wav2Vec2SamePadLayer()\n",
              "      (activation): GELUActivation()\n",
              "    )\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
              "        (attention): Wav2Vec2Attention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Wav2Vec2FeedForward(\n",
              "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "wav2vec = Wav2Vec2Model.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\"\n",
        ").to(CONFIG[\"device\"])\n",
        "wav2vec.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzV59gRL1nFN"
      },
      "source": [
        "## Audio Classifier\n",
        "\n",
        "A feed-forward neural network is used to classify audio embeddings extracted by Wav2Vec 2.0.\n",
        "The network consists of two linear layers with a 256-dimensional hidden layer, ReLU activation, and dropout regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZsL93j_AGDO"
      },
      "outputs": [],
      "source": [
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(AudioClassifier, self).__init__()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(config[\"audio_embedding_dim\"], 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config[\"dropout\"]),\n",
        "            nn.Linear(256, config[\"num_classes\"])\n",
        "        )\n",
        "\n",
        "    def forward(self, audio_emb):\n",
        "        return self.classifier(audio_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2ohmnwV1w__"
      },
      "source": [
        "## Loading model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFtAEoqQJ7Wl",
        "outputId": "169a934e-2d8b-41c5-d29a-600862413db8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AudioClassifier(\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=7, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = torch.load(\n",
        "    CONFIG[\"meld_checkpoint\"],\n",
        "    map_location=CONFIG[\"device\"]\n",
        ")\n",
        "\n",
        "audio_model = AudioClassifier(CONFIG).to(CONFIG[\"device\"])\n",
        "audio_model.load_state_dict(checkpoint[\"model_state\"], strict=True)\n",
        "audio_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O8n2CCXFq0j"
      },
      "source": [
        "## Prediction Distribution Analysis\n",
        "\n",
        "The trained audio model is evaluated on the SEAC test set using the MELD emotion label space.\n",
        "For each test sample, audio embeddings are extracted with the pre-trained Wav2Vec 2.0 encoder and classified using the audio classification head.\n",
        "\n",
        "To analyze model behavior beyond standard performance metrics, the distributions of true and predicted labels are computed.\n",
        "This analysis provides insight into potential class imbalance effects and model bias by comparing the ground-truth label distribution with the distribution of model predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0WAOmSrVam0"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "pred_counter = Counter()\n",
        "target_counter = Counter()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for waveforms, meld_labels in test_loader:\n",
        "\n",
        "        waveforms = waveforms.to(CONFIG[\"device\"])\n",
        "        meld_labels = meld_labels.to(CONFIG[\"device\"])\n",
        "\n",
        "        encoder_outputs = wav2vec(waveforms)\n",
        "        audio_emb = encoder_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        logits = audio_model(audio_emb)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        for t, p in zip(meld_labels, preds):\n",
        "            all_targets.append(int(t))\n",
        "            all_preds.append(int(p))\n",
        "\n",
        "            target_counter[int(t)] += 1\n",
        "            pred_counter[int(p)] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwLmQSL9mDzi",
        "outputId": "8e568767-363f-4eea-a3bb-fe94ddadc0f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribution of TRUE labels (MELD):\n",
            "  neutral (0): 122\n",
            "  joy (1): 116\n",
            "  anger (3): 116\n",
            "  sadness (4): 122\n",
            "  fear (5): 122\n",
            "\n",
            "Distribution of PREDICTED labels:\n",
            "  neutral (0): 598\n"
          ]
        }
      ],
      "source": [
        "inv_meld = {v: k for k, v in CONFIG[\"meld_label_map\"].items()}\n",
        "\n",
        "print(\"Distribution of TRUE labels (MELD):\")\n",
        "for k, v in sorted(target_counter.items()):\n",
        "    print(f\"  {inv_meld.get(k, 'UNK')} ({k}): {v}\")\n",
        "\n",
        "print(\"\\nDistribution of PREDICTED labels:\")\n",
        "for k, v in sorted(pred_counter.items()):\n",
        "    print(f\"  {inv_meld.get(k, 'UNK')} ({k}): {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kORWpJNIISq"
      },
      "source": [
        "## Quantitative Evaluation and Error Analysis\n",
        "\n",
        "Evaluation results show low overall performance of the audio-only model, with particularly poor class-wise discrimination.\n",
        "The confusion matrix and per-class F1-scores reveal a strong bias toward a single predicted class, indicating a collapse in model predictions and motivating further improvements in model design.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIn7dkMUIypn"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix_with_percentages(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    label_id_to_name,\n",
        "    title=\"Confusion Matrix\",\n",
        "    figsize=(8, 6),\n",
        "    cmap=\"Blues\"\n",
        "):\n",
        "    # Determine which labels actually appear\n",
        "    labels = sorted(set(y_true) | set(y_pred))\n",
        "\n",
        "    # Build confusion matrix with explicit labels\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    # Row-wise normalization (recall view)\n",
        "    cm_percent = cm.astype(float) / cm.sum(axis=1, keepdims=True) * 100\n",
        "    cm_percent = np.nan_to_num(cm_percent)\n",
        "\n",
        "    # Build class names dynamically\n",
        "    class_names = [label_id_to_name[l] for l in labels]\n",
        "\n",
        "    # Create annotations\n",
        "    annotations = np.empty_like(cm).astype(str)\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            annotations[i, j] = f\"{cm[i, j]}\\n({cm_percent[i, j]:.1f}%)\"\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=annotations,\n",
        "        fmt=\"\",\n",
        "        cmap=cmap,\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        square=True\n",
        "    )\n",
        "\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.title(title + \" (Counts + Row %)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKgvfO0AJpIp"
      },
      "outputs": [],
      "source": [
        "# Plots per-class F1-scores as a bar chart.\n",
        "def plot_per_class_f1(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    label_id_to_name,\n",
        "    title=\"Per-class F1-score\",\n",
        "    figsize=(8, 5),\n",
        "    ylim=(0.0, 1.0)\n",
        "):\n",
        "\n",
        "    # same labels as confusion matrix\n",
        "    labels = sorted(set(y_true) | set(y_pred))\n",
        "\n",
        "    # compute F1 in fixed label order\n",
        "    f1 = f1_score(y_true, y_pred, labels=labels, average=None, zero_division=0)\n",
        "\n",
        "    # map ids → names\n",
        "    class_names = [label_id_to_name[l] for l in labels]\n",
        "\n",
        "    # plot bars\n",
        "    plt.figure(figsize=figsize)\n",
        "    bars = plt.bar(class_names, f1)\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.ylabel(\"F1-score\")\n",
        "    plt.title(title)\n",
        "    plt.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "    # annotate bars with percentages\n",
        "    for bar, score in zip(bars, f1):\n",
        "        height = bar.get_height()\n",
        "        plt.text(\n",
        "            bar.get_x() + bar.get_width() / 2,\n",
        "            height + 0.02,\n",
        "            f\"{score * 100:.1f}%\",\n",
        "            ha=\"center\",\n",
        "            va=\"bottom\",\n",
        "            fontsize=9\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y01W7MHva5G0",
        "outputId": "6eb483a8-6fbe-47fd-8b75-a585879cc2e7"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(all_targets, all_preds)\n",
        "f1 = f1_score(all_targets, all_preds, average=\"weighted\")\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "cm_sum = np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
        "cm_percent = cm.astype(float) / cm_sum * 100\n",
        "\n",
        "print(f\"SEAC → MELD Audio-only results\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Weighted F1: {f1:.4f}\")\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT4AHJIVK0l9"
      },
      "outputs": [],
      "source": [
        "y_true = np.array(all_targets)\n",
        "y_pred = np.array(all_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc1dc2IvGrPf"
      },
      "source": [
        "The confusion matrix shows that the model predicts all samples as *neutral*, regardless of the true label.\n",
        "This indicates a complete lack of class discrimination and a strong prediction bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "JBgqURozkfO1",
        "outputId": "a736f289-7b59-4586-d200-63a7995c733a"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix_with_percentages(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    label_id_to_name=label_id_to_name,\n",
        "    title=\"Audio model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "Y1LtwFtvKGea",
        "outputId": "a979ac7e-991e-4d41-91ae-85db122043e0"
      },
      "outputs": [],
      "source": [
        "plot_per_class_f1(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    label_id_to_name=label_id_to_name,\n",
        "    title=\"Audio model – F1 per class\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
