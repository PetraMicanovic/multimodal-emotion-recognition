{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOuO3rhuUrJD"
      },
      "source": [
        "# Transfer learning for Speech Emotion Recognition on Serbian acted speech\n",
        "\n",
        "In this notebook, we evaluate a pretrained audio emotion recognition model on a newly collected\n",
        "Serbian acted speech dataset. The experiments are conducted in three stages:\n",
        "\n",
        "1. Baseline evaluation of the pretrained model on two unseen Serbian speakers.\n",
        "2. Fine-tuning the model using the remaining Serbian speakers.\n",
        "3. Re-evaluation on both Serbian and original English test sets.\n",
        "\n",
        "Only the audio modality is considered, as the textual content is not correlated with emotion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paVSsEjfN502"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R33zhUU7lBY",
        "outputId": "e204ca62-9120-4a2b-abe2-d2ded6e7e977"
      },
      "outputs": [],
      "source": [
        "# System setup and dependencies\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install ffmpeg\n",
        "!pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 --index-url https://download.pytorch.org/whl/cu126"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AjCX2je7lBZ",
        "outputId": "f68e25f9-cedc-4a75-c5f2-90f8293ed11a"
      },
      "outputs": [],
      "source": [
        "# Additional Python packages for AI/Audio/NLP\n",
        "\n",
        "!pip -q install transformers datasets accelerate sentencepiece\n",
        "\n",
        "!pip install -q librosa soundfile\n",
        "\n",
        "!pip install torchcodec==0.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jkwuc1l7UfwQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De7y_b71R3NF",
        "outputId": "82c95a98-aee7-4cfd-96fb-bcd53f646b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google Drive is used to store the MELD dataset and to save the processed audio embeddings.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yj_5mMR1RLZ"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p2sGkp1AKXrK"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # ========= DEVICE =========\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "\n",
        "    # ========= PATHS =========\n",
        "    \"seac_root\": Path(\"/content/drive/MyDrive/datasets/data_mono\"),\n",
        "    \"meld_checkpoint\": Path(\"/content/drive/MyDrive/models/checkpoints/audio_best.pt\"),\n",
        "\n",
        "    # ========= DATA =========\n",
        "    \"sample_rate\": 16000,\n",
        "    \"max_audio_length\": 4.0,\n",
        "\n",
        "    # ===== EMOTIONS (APH–SEAC) =====\n",
        "    \"seac_emotion_map\": {\n",
        "        \"0\": \"anger\",\n",
        "        \"1\": \"neutral\",\n",
        "        \"2\": \"happiness\",\n",
        "        \"3\": \"fear\",\n",
        "        \"4\": \"sadness\",\n",
        "    },\n",
        "\n",
        "   # Normalize SEAC → canonical names (MELD-style)\n",
        "    \"emotion_normalization\": {\n",
        "        \"happiness\": \"joy\",\n",
        "        \"neutral\": \"neutral\",\n",
        "        \"anger\": \"anger\",\n",
        "        \"sadness\": \"sadness\",\n",
        "        \"fear\": \"fear\",\n",
        "    },\n",
        "\n",
        "    # MELD label mapping\n",
        "    \"meld_label_map\": {\n",
        "        \"neutral\": 0,\n",
        "        \"joy\": 1,\n",
        "        \"anger\": 3,\n",
        "        \"sadness\": 4,\n",
        "        \"fear\": 5,\n",
        "    },\n",
        "\n",
        "    # ========= Speakers =========\n",
        "    \"selected_speakers\": {\n",
        "        \"0005\":\"female\",\n",
        "        \"1005\":\"male\",\n",
        "    },\n",
        "\n",
        "    # Model\n",
        "    \"audio_embedding_dim\": 768,\n",
        "    \"num_classes\": 7,\n",
        "    \"dropout\": 0.3,\n",
        "\n",
        "\n",
        "    # ========= TRAINING =========\n",
        "    \"batch_size\": 16,\n",
        "    \"lr_finetune\": 1e-4,\n",
        "    \"num_epochs\": 30,\n",
        "    \"early_stopping_patience\": 5,\n",
        "\n",
        "    # ========= REPRODUCIBILITY =========\n",
        "    \"seed\": 42,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nED-MCnBVEpP"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(CONFIG[\"seed\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYoYHltaFSE"
      },
      "outputs": [],
      "source": [
        "label_id_to_name = {\n",
        "    0: \"neutral\",\n",
        "    1: \"joy\",\n",
        "    2: \"surprise\",\n",
        "    3: \"anger\",\n",
        "    4: \"sadness\",\n",
        "    5: \"fear\",\n",
        "    6: \"disgust\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_h6FGVL1V6P"
      },
      "source": [
        "## SEAC  Dataset\n",
        "The SEAC dataset consists of emotionally expressive speech recordings in Serbian, organized hierarchically by speakers and emotion categories. Each speaker is represented by a separate directory containing multiple subfolders, where each subfolder corresponds to a specific emotion and includes several .wav audio files.\n",
        "\n",
        "For the experiments in this work, a subset of the SEAC dataset is used, defined by a selected set of speakers.\n",
        "\n",
        "Audio samples are loaded by iterating over the selected speakers and their emotion-specific subfolders. Emotion labels are first normalized and then mapped to the MELD emotion label space in order to ensure label consistency across datasets. Each sample is stored together with its audio path, normalized emotion, numerical class label, and speaker identifier.\n",
        "\n",
        "To enable model evaluation, a custom PyTorch dataset is defined for audio processing. All audio signals are converted to mono, resampled to 16 kHz, and truncated to a fixed maximum duration. The test set consists of all loaded SEAC samples and is evaluated using a DataLoader with a batch size of one, allowing sample-level analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "m0VN18RBsrnJ"
      },
      "outputs": [],
      "source": [
        "def load_seac_samples(config, speakers):\n",
        "    samples = []\n",
        "\n",
        "    for speaker_id in speakers:\n",
        "        speaker_path = config[\"seac_root\"] / speaker_id\n",
        "\n",
        "        for folder_id, emotion in config[\"seac_emotion_map\"].items():\n",
        "            emotion_path = speaker_path / folder_id\n",
        "            if not emotion_path.exists():\n",
        "                continue\n",
        "\n",
        "            for wav in emotion_path.glob(\"*.wav\"):\n",
        "\n",
        "                # 1. normalize emotion name\n",
        "                norm_emotion = config[\"emotion_normalization\"][emotion]\n",
        "\n",
        "                # 2. map to MELD label\n",
        "                meld_label = config[\"meld_label_map\"][norm_emotion]\n",
        "\n",
        "                samples.append({\n",
        "                    \"audio_path\": wav,\n",
        "                    \"emotion\": norm_emotion,\n",
        "                    \"label\": meld_label,\n",
        "                    \"speaker\": speaker_id\n",
        "                })\n",
        "\n",
        "    return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CixL0pjj0eNl"
      },
      "outputs": [],
      "source": [
        "class SEACAudioDataset(Dataset):\n",
        "    def __init__(self, samples, sample_rate=16000, max_len=4.0):\n",
        "        self.samples = samples\n",
        "        self.sample_rate = sample_rate\n",
        "        self.max_len = int(sample_rate * max_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        path = sample[\"audio_path\"]\n",
        "        label = sample[\"label\"]\n",
        "\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(path)\n",
        "\n",
        "            if sr != self.sample_rate:\n",
        "                waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
        "\n",
        "            waveform = waveform.squeeze(0)\n",
        "\n",
        "            if waveform.shape[0] > self.max_len:\n",
        "                waveform = waveform[:self.max_len]\n",
        "            else:\n",
        "                pad = self.max_len - waveform.shape[0]\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, pad))\n",
        "\n",
        "        except Exception:\n",
        "            waveform = torch.zeros(self.max_len, dtype=torch.float32)\n",
        "\n",
        "        return waveform, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QGfaYlZD_DiL"
      },
      "outputs": [],
      "source": [
        "def build_audio_loader(\n",
        "    samples,\n",
        "    config,\n",
        "    batch_size,\n",
        "    shuffle,\n",
        "    collate_fn=None\n",
        "):\n",
        "    dataset = SEACAudioDataset(\n",
        "        samples,\n",
        "        sample_rate=config[\"sample_rate\"],\n",
        "        max_len=config[\"max_audio_length\"]\n",
        "    )\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kd7n1NHZ_G-2"
      },
      "outputs": [],
      "source": [
        "def audio_collate_fn(batch):\n",
        "    waveforms, labels = zip(*batch)\n",
        "\n",
        "    waveforms = [\n",
        "        torch.tensor(w, dtype=torch.float32)\n",
        "        if not torch.is_tensor(w) else w\n",
        "        for w in waveforms\n",
        "    ]\n",
        "\n",
        "    waveforms = torch.nn.utils.rnn.pad_sequence(\n",
        "        waveforms,\n",
        "        batch_first=True\n",
        "    )\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return waveforms, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ioh7RCtRWazJ",
        "outputId": "1ca4b48b-6a70-4dfd-bc6b-5658358ab5ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 598 SEAC test samples\n",
            "Counter({0: 122, 5: 122, 4: 122, 3: 116, 1: 116})\n",
            "Counter({'neutral': 122, 'fear': 122, 'sadness': 122, 'anger': 116, 'joy': 116})\n"
          ]
        }
      ],
      "source": [
        "samples_step1 = load_seac_samples(CONFIG,CONFIG[\"selected_speakers\"])\n",
        "print(f\"Loaded {len(samples_step1)} SEAC test samples\")\n",
        "\n",
        "from collections import Counter\n",
        "print(Counter([s[\"label\"] for s in samples_step1]))\n",
        "print(Counter([s[\"emotion\"] for s in samples_step1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y4YFSgy-VHLX"
      },
      "outputs": [],
      "source": [
        "test_loader = build_audio_loader(\n",
        "    samples=samples_step1,\n",
        "    config=CONFIG,\n",
        "    batch_size=1,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "941kUa1SEbNP"
      },
      "source": [
        "## Audio Feature Extraction\n",
        "\n",
        "A pre-trained Wav2Vec 2.0 base model is used to extract audio embeddings from raw waveforms.\n",
        "The model is initialized with pre-trained weights and used in evaluation mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aAtG7-TPbxx8",
        "outputId": "af0aa40f-3f26-4c49-b8fb-126b23a802a5"
      },
      "outputs": [],
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "wav2vec = Wav2Vec2Model.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\"\n",
        ").to(CONFIG[\"device\"])\n",
        "wav2vec.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzV59gRL1nFN"
      },
      "source": [
        "## Audio Classifier\n",
        "A feed-forward neural network is used to classify audio embeddings extracted by Wav2Vec 2.0.\n",
        "The network consists of two linear layers with a 256-dimensional hidden layer, ReLU activation, and dropout regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZsL93j_AGDO"
      },
      "outputs": [],
      "source": [
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(AudioClassifier, self).__init__()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(config[\"audio_embedding_dim\"], 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config[\"dropout\"]),\n",
        "            nn.Linear(256, config[\"num_classes\"])\n",
        "        )\n",
        "\n",
        "    def forward(self, audio_emb):\n",
        "        return self.classifier(audio_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQULIZMtPeyF"
      },
      "source": [
        "## Loading model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "pFtAEoqQJ7Wl",
        "outputId": "c6ac9c44-1e33-4939-92ed-296246056ce4"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\n",
        "    CONFIG[\"meld_checkpoint\"],\n",
        "    map_location=CONFIG[\"device\"]\n",
        ")\n",
        "\n",
        "audio_model = AudioClassifier(CONFIG).to(CONFIG[\"device\"])\n",
        "audio_model.load_state_dict(checkpoint[\"model_state\"], strict=True)\n",
        "audio_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2ohmnwV1w__"
      },
      "source": [
        "## Prediction Distribution Analysis\n",
        "\n",
        "The trained audio model is evaluated on the SEAC test set using the MELD emotion label space.\n",
        "For each test sample, audio embeddings are extracted with the pre-trained Wav2Vec 2.0 encoder and classified using the audio classification head.\n",
        "\n",
        "To analyze model behavior beyond standard performance metrics, the distributions of true and predicted labels are computed.\n",
        "This analysis provides insight into potential class imbalance effects and model bias by comparing the ground-truth label distribution with the distribution of model predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def temporal_pooling(hidden_states):\n",
        "    mean_pool = hidden_states.mean(dim=1)\n",
        "    std_pool  = hidden_states.std(dim=1)\n",
        "    return mean_pool + std_pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0WAOmSrVam0"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "pred_counter = Counter()\n",
        "target_counter = Counter()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for waveforms, meld_labels in test_loader:\n",
        "\n",
        "        waveforms = waveforms.to(CONFIG[\"device\"])\n",
        "        meld_labels = meld_labels.to(CONFIG[\"device\"])\n",
        "\n",
        "        encoder_outputs = wav2vec(waveforms)\n",
        "        hidden_states = encoder_outputs.last_hidden_state\n",
        "        audio_emb = temporal_pooling(hidden_states)\n",
        "\n",
        "        logits = audio_model(audio_emb)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        for t, p in zip(meld_labels, preds):\n",
        "            all_targets.append(int(t))\n",
        "            all_preds.append(int(p))\n",
        "\n",
        "            target_counter[int(t)] += 1\n",
        "            pred_counter[int(p)] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwLmQSL9mDzi",
        "outputId": "f14167b1-cf99-45cb-ec8f-1a9f877d33ac"
      },
      "outputs": [],
      "source": [
        "inv_meld = {v: k for k, v in CONFIG[\"meld_label_map\"].items()}\n",
        "\n",
        "print(\"Distribution of TRUE labels (MELD):\")\n",
        "for k, v in sorted(target_counter.items()):\n",
        "    print(f\"  {inv_meld.get(k, 'UNK')} ({k}): {v}\")\n",
        "\n",
        "print(\"\\nDistribution of PREDICTED labels:\")\n",
        "for k, v in sorted(pred_counter.items()):\n",
        "    print(f\"  {inv_meld.get(k, 'UNK')} ({k}): {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kORWpJNIISq"
      },
      "source": [
        "## Prediction Distribution Analysis\n",
        "\n",
        "The pretrained audio emotion recognition model, trained on the MELD dataset, is evaluated on the SEAC test set using the MELD emotion label space.\n",
        "Audio features are extracted using the Wav2Vec 2.0 encoder and classified by the audio classification head without any additional fine-tuning on Serbian data.\n",
        "\n",
        "Beyond standard performance metrics (Accuracy = 0.2492, Weighted F1 = 0.1425), we analyze the distributions of true and predicted labels to better understand the model’s behavior under cross-lingual transfer.\n",
        "In particular, comparing the ground-truth label distribution with the distribution of predicted labels reveals the impact of domain mismatch and highlights potential prediction bias.\n",
        "\n",
        "The confusion matrix shows a strong tendency of the model to predict a limited subset of emotions, most notably *neutral* and *anger*, while some classes (e.g., *disgust*) are never predicted.\n",
        "This indicates that the pretrained model does not generalize well to unseen Serbian speakers and that significant class-wise performance degradation occurs due to language and speaker mismatch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIn7dkMUIypn"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix_with_percentages(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    label_id_to_name,\n",
        "    title=\"Confusion Matrix\",\n",
        "    figsize=(8, 6),\n",
        "    cmap=\"Blues\"\n",
        "):\n",
        "    # Determine which labels actually appear\n",
        "    labels = sorted(set(y_true) | set(y_pred))\n",
        "\n",
        "    # Build confusion matrix with explicit labels\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    # Row-wise normalization (recall view)\n",
        "    cm_percent = cm.astype(float) / cm.sum(axis=1, keepdims=True) * 100\n",
        "    cm_percent = np.nan_to_num(cm_percent)\n",
        "\n",
        "    # Build class names dynamically\n",
        "    class_names = [label_id_to_name[l] for l in labels]\n",
        "\n",
        "    # Create annotations\n",
        "    annotations = np.empty_like(cm).astype(str)\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            annotations[i, j] = f\"{cm[i, j]}\\n({cm_percent[i, j]:.1f}%)\"\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=annotations,\n",
        "        fmt=\"\",\n",
        "        cmap=cmap,\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        square=True\n",
        "    )\n",
        "\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.title(title + \" (Counts + Row %)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKgvfO0AJpIp"
      },
      "outputs": [],
      "source": [
        "# Plots per-class F1-scores as a bar chart.\n",
        "def plot_per_class_f1(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    label_id_to_name,\n",
        "    title=\"Per-class F1-score\",\n",
        "    figsize=(8, 5),\n",
        "    ylim=(0.0, 1.0)\n",
        "):\n",
        "\n",
        "    # same labels as confusion matrix\n",
        "    labels = sorted(set(y_true) | set(y_pred))\n",
        "\n",
        "    # compute F1 in fixed label order\n",
        "    f1 = f1_score(y_true, y_pred, labels=labels, average=None, zero_division=0)\n",
        "\n",
        "    # map ids → names\n",
        "    class_names = [label_id_to_name[l] for l in labels]\n",
        "\n",
        "    # plot bars\n",
        "    plt.figure(figsize=figsize)\n",
        "    bars = plt.bar(class_names, f1)\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.ylabel(\"F1-score\")\n",
        "    plt.title(title)\n",
        "    plt.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "    # annotate bars with percentages\n",
        "    for bar, score in zip(bars, f1):\n",
        "        height = bar.get_height()\n",
        "        plt.text(\n",
        "            bar.get_x() + bar.get_width() / 2,\n",
        "            height + 0.02,\n",
        "            f\"{score * 100:.1f}%\",\n",
        "            ha=\"center\",\n",
        "            va=\"bottom\",\n",
        "            fontsize=9\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y01W7MHva5G0",
        "outputId": "5231b227-aa52-4c39-fa44-29c1367ad880"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(all_targets, all_preds)\n",
        "f1 = f1_score(all_targets, all_preds, average=\"weighted\")\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "cm_sum = np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
        "cm_percent = cm.astype(float) / cm_sum * 100\n",
        "\n",
        "print(f\"SEAC → MELD Audio-only results\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Weighted F1: {f1:.4f}\")\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT4AHJIVK0l9"
      },
      "outputs": [],
      "source": [
        "y_true = np.array(all_targets)\n",
        "y_pred = np.array(all_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcSWXyykS0qx"
      },
      "source": [
        "\n",
        "\n",
        "A strong prediction bias toward the **neutral** and **anger** classes can be observed.  \n",
        "Most samples from different true emotion categories are mapped to these two labels, indicating limited discriminative ability under cross-lingual transfer.\n",
        "\n",
        "For the **neutral** class, 73.8% of the samples are correctly classified, while the remaining instances are mainly misclassified as **anger** (26.2%).  \n",
        "However, other emotions such as **joy**, **sadness**, and **fear** are predominantly misclassified as either **neutral** or **anger**, demonstrating substantial class confusion.\n",
        "\n",
        "Notably, the model fails to predict certain classes entirely.  \n",
        "The **disgust** class is never predicted, and the **surprise** class appears only marginally, reflecting severe class-wise degradation when transferring from English (MELD) to Serbian (SEAC) speech data.\n",
        "\n",
        "Overall, the confusion matrix confirms that the pretrained MELD audio model does not generalize well to unseen Serbian speakers.  \n",
        "The dominance of a small subset of predicted classes and the absence of meaningful predictions for others highlight the impact of language mismatch and motivate the need for speaker- and language-specific fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "JBgqURozkfO1",
        "outputId": "1f83e924-e5a6-45b8-e06e-25d67ce98e09"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix_with_percentages(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    label_id_to_name=label_id_to_name,\n",
        "    title=\"Audio model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "Y1LtwFtvKGea",
        "outputId": "d2936eb2-1e65-4f2a-a117-841e4538784e"
      },
      "outputs": [],
      "source": [
        "plot_per_class_f1(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    label_id_to_name=label_id_to_name,\n",
        "    title=\"Audio model – F1 per class\"\n",
        ")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
